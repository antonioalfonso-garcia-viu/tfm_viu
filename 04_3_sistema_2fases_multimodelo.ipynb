{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sklearn\n",
    "from scipy import stats\n",
    "\n",
    "# importar fichero con utilidades propias\n",
    "from commons import myfunctions as myfunc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se indican parámetros necesarios en el notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "SEMILLA = 42\n",
    "B_TIPO=\"bin_s\"\n",
    "M_TIPO=\"mul_s\"\n",
    "DATA_TIPO=\"mul_s\"\n",
    "PRE_DATA_FILE=  \"rows_transpose_norm_by_gene_id_with_target_num_\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre del fichero con las muestras a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE= PRE_DATA_FILE + DATA_TIPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incializar variables, comprobar entorno y leer el fichero con las muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "myfunc.reset_vars()\n",
    "\n",
    "myfunc.NOTEBK_FILENAME = myfunc.get_nb_name()\n",
    "myfunc.EXEC_DIR = os.path.join(myfunc.EXEC_DIR, M_TIPO)\n",
    "\n",
    "myfunc.check_enviroment(myfunc.DATA_DIR, myfunc.CFDNA_DIR, myfunc.GENCODE_DIR, myfunc.H5_DIR, myfunc.LOG_DIR, myfunc.CSV_DIR, myfunc.MODEL_DIR, myfunc.EXEC_DIR, myfunc.MET_DIR)\n",
    "\n",
    "def mutual_info_classif_state(X, y):\n",
    "    return mutual_info_classif(X, y, random_state=SEMILLA)\n",
    "\n",
    "#  leer fichero de datos\n",
    "df_t = myfunc.read_h5_to_df(DATA_FILE, myfunc.H5_DIR)\n",
    "display(df_t.groupby(\"target\").size())\n",
    "print(\"Shape df:\",df_t.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se separan las muestras utilizando la misma semilla que se utilizó para el entrenamiento del modelo.\n",
    "\n",
    "Se utilizan las muestras de _test_ para evaluar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "\n",
    "X = df_t.iloc[:, :-1]\n",
    "y = df_t.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "y_test_bin = y_test.copy()\n",
    "y_test_bin[y_test_bin != 0] = 1\n",
    "\n",
    "# y_train_bin = y_train.copy()\n",
    "# y_train_bin[y_train_bin != 0] = 1\n",
    "\n",
    "print(\"Shape X_test:\",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer fichero con métricas calculadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo1= os.path.join(myfunc.MET_DIR, \"metricas_0042.csv\")\n",
    "\n",
    "fichero1 = os.path.basename(archivo1)\n",
    "carpeta1 = os.path.dirname(archivo1)\n",
    "\n",
    "df_tmp = myfunc.read_csv_to_df_spa(fichero1, carpeta1)[['tipo', 'select', 'clasific', 'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'roc_auc_ovr','fichero_modelo','indices_auc','indices_jaccard']]\n",
    "display(df_tmp.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccion de modelos para clasificación binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escoger los mejores para la clasificación binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df1 = df_tmp[df_tmp.tipo.str.startswith(B_TIPO)].copy()\n",
    "print(df1.columns)\n",
    "\n",
    "display(df1.shape)\n",
    "display(df1.sample(3)[[\"tipo\",\"select\",\"clasific\",\"accuracy\",\"roc_auc\",\"fichero_modelo\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscar los 3 mejores modelos para clasificación binaria por auc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_modelos_escoger = 3\n",
    "\n",
    "df3 = df1.copy()\n",
    "ficheros_modelos_bin_auc = []\n",
    "ficheros_modelos_bin_auc1 =[]\n",
    "for lista1 in df3.sort_values(by=[\"roc_auc\",\"accuracy\"], ascending=False)[\"fichero_modelo\"].head(num_modelos_escoger):\n",
    "    ficheros_modelos_bin_auc1.append(lista1)\n",
    "ficheros_modelos_bin_auc.append(ficheros_modelos_bin_auc1)\n",
    "\n",
    "print(ficheros_modelos_bin_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccion de modelos para clasificación múltiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar los indices de auc y de jaccard en columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df1 = df_tmp[df_tmp.tipo.str.startswith(M_TIPO)].copy()\n",
    "\n",
    "# Usando apply junto con pd.Series para expandir los arrays en columnas separadas.\n",
    "df1['auc_list'] = df1['indices_auc'].apply(lambda x: ast.literal_eval(x))\n",
    "df1['jac_list'] = df1['indices_jaccard'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "auc_expanded = df1['auc_list'].apply(pd.Series)\n",
    "auc_expanded.columns = ['auc_0', 'auc_1', 'auc_2', 'auc_3', 'auc_4', 'auc_5', 'auc_6']\n",
    "auc_expanded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "jac_expanded = df1['jac_list'].apply(pd.Series)\n",
    "jac_expanded.columns = ['jac_0', 'jac_1', 'jac_2', 'jac_3', 'jac_4', 'jac_5', 'jac_6']\n",
    "jac_expanded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df2 = pd.concat([df1, auc_expanded], axis=1)\n",
    "\n",
    "df2 = pd.concat([df2, jac_expanded], axis=1)\n",
    "\n",
    "df2.drop(columns=['auc_list','roc_auc','jac_list'], inplace=True)\n",
    "\n",
    "display(df2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscar los 3 mejores modelos por auc por cada tipo de cáncer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escogiendo los modelos por tipo\n",
    "df3 = df2[df2.tipo==M_TIPO]\n",
    "\n",
    "#  escoger el mejor modelos de cada clase pero de los diferentes algoritmos de seleccion\n",
    "num_modelos_escoger_select = 3\n",
    "ficheros_modelos_mul_auc = []\n",
    "for clase1 in range(len(myfunc.TARGETS)):\n",
    "    ficheros_modelos_mul_auc_tipo = []\n",
    "    for lista1 in df3.sort_values(by=[f\"auc_{clase1}\",\"roc_auc_ovr\"], ascending=False)[\"fichero_modelo\"].head(num_modelos_escoger_select):\n",
    "        ficheros_modelos_mul_auc_tipo.append(lista1)\n",
    "    ficheros_modelos_mul_auc.append(ficheros_modelos_mul_auc_tipo)\n",
    "display(ficheros_modelos_mul_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leer modelos guardados de clasificación binaria y multiclase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ficheros_modelos_bin = ficheros_modelos_bin_auc\n",
    "print(ficheros_modelos_bin_auc)\n",
    "\n",
    "modelos_bin = []\n",
    "for modelo_x in ficheros_modelos_bin:\n",
    "    for modelo_x_1 in modelo_x:\n",
    "        fichero1 = modelo_x_1\n",
    "        modelos_bin.append(myfunc.read_modelo(myfunc.MODEL_DIR, fichero1))\n",
    "\n",
    "ficheros_modelos_mul = ficheros_modelos_mul_auc\n",
    "print(ficheros_modelos_mul_auc)\n",
    "\n",
    "modelos_mul = []\n",
    "for modelos_x in ficheros_modelos_mul:\n",
    "    modelos = []\n",
    "    for modelo_x_1 in modelos_x:        \n",
    "        fichero1 = modelo_x_1\n",
    "        modelos.append(myfunc.read_modelo(myfunc.MODEL_DIR, fichero1))\n",
    "    modelos_mul.append(modelos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 1. clasificación binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace la predicción para cada modelo seleccionado y se queda con la moda, valores más frecuentes, de la predicción de cada muestra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def consenso_bin(modelos_bin, X):\n",
    "#   predicciones = []\n",
    "#   for modelo1 in modelos_bin:\n",
    "#     predict_bin = modelo1.predict(X)\n",
    "#     predicciones.append(predict_bin)\n",
    "\n",
    "#   # Convertir la lista de todas las predicciones en un array de numpy\n",
    "#   todas_predicciones_array = np.array(predicciones)\n",
    "\n",
    "#   # consenso_predicciones, _ = stats.mode(todas_predicciones_array, axis=0, keepdims=True)\n",
    "#   consenso_predicciones, _ = stats.mode(todas_predicciones_array, axis=0)\n",
    "\n",
    "#   return list(consenso_predicciones[0])\n",
    "\n",
    "# predicciones_bin = consenso_bin(modelos_bin, X_test)\n",
    "\n",
    "predicciones = []\n",
    "for modelo1 in modelos_bin:\n",
    "  predicciones.append(modelo1.predict(X_test))\n",
    "\n",
    "# Convertir la lista de todas las predicciones en un array de numpy\n",
    "todas_predicciones_array = np.array(predicciones)\n",
    "\n",
    "# consenso_predicciones, _ = stats.mode(todas_predicciones_array, axis=0, keepdims=True)\n",
    "consenso_predicciones, _ = stats.mode(todas_predicciones_array, axis=0)\n",
    "\n",
    "predicciones_bin = list(consenso_predicciones[0])\n",
    "\n",
    "print(sklearn.metrics.classification_report(y_test_bin, predicciones_bin))\n",
    "\n",
    "conf_matrix = sklearn.metrics.confusion_matrix(y_test_bin, predicciones_bin)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test_bin, predicciones_bin)\n",
    "precision = sklearn.metrics.precision_score(y_test_bin, predicciones_bin, average=\"macro\")\n",
    "recall = sklearn.metrics.recall_score(y_test_bin, predicciones_bin, average=\"macro\")\n",
    "f1 = sklearn.metrics.f1_score(y_test_bin, predicciones_bin, average=\"macro\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"           Exactitud: %.6f\" % (accuracy)) \n",
    "print(\"   Precisión (media): %.6f\" % (precision))\n",
    "print(\"      Recall (media): %.6f\" % (recall))\n",
    "print(\"    F1-score (media): %.6f\" % (f1))\n",
    "\n",
    "myfunc.ver_metricas_bin_matriz_confusion(y_test_bin, predicciones_bin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 2. clasificación multiclase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Haciendo un consenso por cada tipo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tener en cuenta probabilidades ya calculadas para posteriores iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el array para almacenar la clase de consenso y las probabilidades máximas asociadas para cada clase\n",
    "consenso_clases = np.full(len(X_test), -1, dtype=int)  # Inicializar con -1 para indicar que no hay predicción todavía\n",
    "consenso_proba_max_por_clase = np.zeros((len(X_test), len(modelos_mul)))  # Una columna para cada clase\n",
    "\n",
    "# Almacenar todas las probabilidades de todos los modelos\n",
    "probabilidades_todas = []\n",
    "\n",
    "# Por cada tipo de cáncer se itera sobre los modelos escogidos que mejor predicen ese tipo\n",
    "for i, conjunto_modelos in enumerate(modelos_mul):\n",
    "    # Almacenar las probabilidades para este tipo de cáncer\n",
    "    probabilidades_tipo = []\n",
    "    probabilidades_proba_tipo = []\n",
    "    \n",
    "    # Obtener las probabilidades de cada modelo en el conjunto para cada muestra\n",
    "    for modelo in conjunto_modelos:\n",
    "        \n",
    "        probabilidades1 = modelo.predict(X_test)\n",
    "        probabilidades_tipo.append(probabilidades1)\n",
    "        \n",
    "        probabilidades_todas.append(probabilidades1)\n",
    "\n",
    "        probabilidades_proba = modelo.predict_proba(X_test)\n",
    "        probabilidades_proba_tipo.append(probabilidades_proba)\n",
    "    \n",
    "    # Calcular la media de las probabilidades para obtener la probabilidad de consenso \n",
    "    # consenso_predicciones, _ = stats.mode(probabilidades_tipo, axis=0, keepdims=True)\n",
    "    consenso_predicciones, _ = stats.mode(probabilidades_tipo, axis=0)\n",
    "    consenso_tipo = consenso_predicciones[0]\n",
    "\n",
    "    consenso_proba_tipo = np.mean(probabilidades_proba_tipo, axis=0)[:,i]\n",
    "\n",
    "    # Actualizar las clases de consenso y las probabilidades máximas solo para la clase actual (i)\n",
    "    for j in range(len(X_test)):\n",
    "        # Solo actualizamos si la nueva probabilidad es mayor que la máxima anterior para la que ha predicho como clase i\n",
    "        if consenso_tipo[j] == i:\n",
    "            if consenso_proba_tipo[j] > consenso_proba_max_por_clase[j, i]:\n",
    "                consenso_proba_max_por_clase[j, i] = consenso_proba_tipo[j]\n",
    "                consenso_clases[j] = i\n",
    "    \n",
    "# Con todas las probabilidades guardadas se calculan las modas por si queda alguna pendiente\n",
    "# consenso_predicciones_todas, _ = stats.mode(probabilidades_todas, axis=0, keepdims=True)\n",
    "consenso_predicciones_todas, _ = stats.mode(probabilidades_todas, axis=0)\n",
    "\n",
    "# Si queda algurna pendiente, me quedo con la moda de todas las probabilidades guardadas\n",
    "for j in range(len(X_test)):\n",
    "    if consenso_clases[j] == -1:\n",
    "        consenso_clases[j] = consenso_predicciones_todas[0][j]\n",
    "\n",
    "predicciones_mul = list(consenso_clases)\n",
    "\n",
    "#  Para el consenso final se conservan las prediciones del modelo de clasificación binaria\n",
    "consenso_final = [predicciones_mul[i] if predicciones_bin[i] == 1 else 0 for i in range(len(predicciones_mul))]\n",
    "\n",
    "y_pred = consenso_final.copy()\n",
    "\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred, zero_division=0, digits=3))\n",
    "\n",
    "conf_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "precision = sklearn.metrics.precision_score(y_test, y_pred, average=\"macro\")\n",
    "recall = sklearn.metrics.recall_score(y_test, y_pred, average=\"macro\")\n",
    "f1 = sklearn.metrics.f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"           Exactitud: %.6f\" % (accuracy)) \n",
    "print(\"   Precisión (media): %.6f\" % (precision))\n",
    "print(\"      Recall (media): %.6f\" % (recall))\n",
    "print(\"    F1-score (media): %.6f\" % (f1))\n",
    "\n",
    "\n",
    "myfunc.ver_metricas_multi_matriz_confusion(y_test,y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalización del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.datetime.now()\n",
    "total_time = end_time - start_time\n",
    "myfunc.verbose(f\"Notebook ha tardado {total_time} segundos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
