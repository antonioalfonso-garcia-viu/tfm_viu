{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas de las semillas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al estar las muestras equilibradas para el entramiento de los modelos, la exactitud (accuracy), la precisión (precision), la sensibilidad (recall) y la puntuación F1 adquieren más relevancia y se vuelven más confiables para comparar el rendimiento de los modelos. \n",
    "\n",
    "El equilibrio de las clases reduce el sesgo hacia la clase mayoritaria, que es una de las principales limitaciones de estas métricas en conjuntos de datos desequilibrados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import psutil\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from commons import myfunctions as myfunc\n",
    "\n",
    "start_time = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leer fichero con métricas de los modelos entrenados con varias semillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo1= os.path.join(myfunc.MET_DIR, \"metricas_semillas.csv\")\n",
    "\n",
    "fichero1 = os.path.basename(archivo1)\n",
    "carpeta1 = os.path.dirname(archivo1)\n",
    "\n",
    "df_tmp = myfunc.read_csv_to_df_spa(fichero1, carpeta1)[['tipo', 'select', 'clasific', 'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc', 'roc_auc_ovr','fichero_modelo','indices_auc','indices_jaccard']]\n",
    "df_tmp['tipo_select_clasific'] = df_tmp['tipo'] + '-' + df_tmp['select'] + '-' + df_tmp['clasific']\n",
    "df_tmp['select_clasific'] = df_tmp['select'] + '-' + df_tmp['clasific']\n",
    "df_tmp['auc'] = df_tmp['roc_auc'].fillna(0) + df_tmp['roc_auc_ovr'].fillna(0)\n",
    "\n",
    "df_c = df_tmp.copy()\n",
    "df1 = df_c.sort_values([\"tipo\",\"select\",\"clasific\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobar la gráfica con la métrica de la exactitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "media_accuracy = df1.groupby(['tipo', 'select_clasific'])['accuracy'].mean().reset_index()\n",
    "\n",
    "for i, tipo in enumerate(df1['tipo'].unique()):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    current_palette = sns.color_palette(\"muted\", n_colors=len(df1['select_clasific'].unique()))\n",
    "    sns.boxplot(x='select_clasific', y='accuracy', data=df1[df1.tipo == tipo], ax=ax, palette=current_palette)\n",
    "    ax.set_title(f'Tipo: {tipo}-{myfunc.EXAMPLES_MAPPING[tipo]}')\n",
    "\n",
    "    # Encontrar el 'select_clasific' con la mejor media de auc para este 'tipo'\n",
    "    mejor_media = media_accuracy[media_accuracy['tipo'] == tipo]['accuracy'].max()\n",
    "\n",
    "    # Comprobar que hay una mejor media antes de continuar\n",
    "    if not media_accuracy[(media_accuracy['tipo'] == tipo) & (media_accuracy['accuracy'] == mejor_media)].empty:\n",
    "        mejor_select_clasific = media_accuracy[(media_accuracy['tipo'] == tipo) & (media_accuracy['accuracy'] == mejor_media)]['select_clasific'].values[0]\n",
    "\n",
    "        # Obtener la posición x de la mejor media\n",
    "        x_posicion_mejor_media = df1[df1['tipo'] == tipo]['select_clasific'].unique().tolist().index(mejor_select_clasific)\n",
    "\n",
    "        # Añadir la línea vertical punteada en la posición de la mejor media\n",
    "        ax.axvline(x=x_posicion_mejor_media, color='red', linestyle='--', lw=1)\n",
    "\n",
    "        # Añadir la anotación de la mejor media\n",
    "        ax.annotate(f'Mejor: {mejor_media:.4f}', xy=(x_posicion_mejor_media, mejor_media),\n",
    "                    xytext=(50, 30), textcoords='offset points',\n",
    "                    arrowprops=dict(facecolor='black', shrink=0.1), ha='center')\n",
    "    \n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Número de veces que se ha entrenado cada modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- por cada 4 tipo de conjunto de datos:\n",
    "\n",
    "    -  se han entrenado para los tres algoritmos de selección (ANOVA, MI, RF) \n",
    "    \n",
    "        - y por cada uno de esos 3, se han utilizado los 5 de clasificación(LR-EN, LR-L1, LR-L2, RN, SVM)\n",
    "\n",
    "En total 60 por cada semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_c.sort_values([\"tipo\",\"select\",\"clasific\"])\n",
    "df_conta = df1.groupby(\"tipo_select_clasific\").size()\n",
    "\n",
    "myfunc.verbose(f\"Modelos distintos por los 4 conjunto de datos: {len(df_conta)}\")\n",
    "\n",
    "myfunc.verbose(\"Numero de veces que se ha entrenado cada modelo:\")\n",
    "display(df_conta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Las medias y desviaciones de cada modelo por cada métrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_c.sort_values([\"tipo\",\"select\",\"clasific\"])\n",
    "\n",
    "columnas = [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auc\"]\n",
    "\n",
    "df_media = df1.groupby(\"tipo_select_clasific\")[columnas].mean()\n",
    "df_std = df1.groupby(\"tipo_select_clasific\")[columnas].std()\n",
    "df_suma = df_media + df_std\n",
    "\n",
    "df_merge = df_media.merge(df_std, left_index=True, right_index=True)\n",
    "df_merge = df_merge.merge(df_suma, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas para los modelos de clasificación binaria con muestras reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc.verbose(\"Media de los modelos con el conjunto de datos bin_s, solo muestras reales\")\n",
    "\n",
    "display(len(df_merge))\n",
    "display(df_merge[df_merge.index.str.startswith(\"bin_s\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas para los modelos de clasificación binaria con muestras reales y sintéticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc.verbose(\"Media de los modelos con el conjunto de datos bin_m, muestras reales y sintéticas\")\n",
    "\n",
    "display(len(df_merge))\n",
    "display(df_merge[df_merge.index.str.startswith(\"bin_m\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas para los modelos de clasificación multicáncer con muestras reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc.verbose(\"Media de los modelos con el conjunto de datos mul_s, solo muestras reales\")\n",
    "\n",
    "display(len(df_merge))\n",
    "display(df_merge[df_merge.index.str.startswith(\"mul_s\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas para los modelos de clasificación multicáncer con muestras reales y sintéticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc.verbose(\"Media de los modelos con el conjunto de datos mul_s, muestras reales y sintéticas\")\n",
    "\n",
    "display(len(df_merge))\n",
    "display(df_merge[df_merge.index.str.startswith(\"mul_m\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columnas que interesan y funciones para mínimos para las desviaciones y máximos para el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_media = [\"accuracy_x\", \"precision_x\", \"recall_x\", \"f1_score_x\", \"auc_x\"] \n",
    "columns_std = [\"accuracy_y\", \"precision_y\", \"recall_y\", \"f1_score_y\", \"auc_y\"]\n",
    "columns_suma = [\"accuracy\", \"precision\", \"recall\", \"f1_score\", \"auc\"]\n",
    "\n",
    "\n",
    "def get_max_by_column(df):\n",
    "  max_by_column = {}\n",
    "  for column in df.columns:\n",
    "    max_by_column[column] = df[column].idxmax(), df[column].max()\n",
    "\n",
    "  return max_by_column\n",
    "\n",
    "\n",
    "def get_min_by_column(df):\n",
    "  min_by_column = {}\n",
    "  for column in df.columns:\n",
    "    min_by_column[column] = df[column].idxmin(), df[column].min()\n",
    "\n",
    "  return min_by_column\n",
    "\n",
    "\n",
    "def mostrar_mejores(tipo1, estadistico1, df1, orden_columnas=[\"accuracy\"], ordenacion1=False):\n",
    "  df_tipo = df1.query(f\"tipo_select_clasific.str.startswith('{tipo1}')\")\n",
    "  if ordenacion1 == False:\n",
    "    # búsqueda de la mayor media\n",
    "    valores1 = pd.DataFrame(get_max_by_column(df_tipo))\n",
    "  else:\n",
    "    # búsqueda de la menor desviación\n",
    "    valores1 = pd.DataFrame(get_min_by_column(df_tipo))\n",
    "\n",
    "  algoritmos = valores1.iloc[0].unique()\n",
    "  columnas = df_tipo.columns\n",
    "  df_tipo = df_tipo.round(4)\n",
    "  \n",
    "  myfunc.verbose(f\"Algoritmos en primera posicion por alguna de sus columnas para conjunto de datos {tipo1} y estadístico {estadistico1}\")\n",
    "\n",
    "  display(df_tipo[df_tipo.index.isin(algoritmos)].sort_values(orden_columnas, ascending=ordenacion1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumen de los mejores por accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orden_columnas = [\"accuracy_x\"]\n",
    "tipo1 = 'bin_s'\n",
    "mostrar_mejores(tipo1, \"media\", df_merge[columns_media], orden_columnas)\n",
    "tipo1 = 'bin_m'\n",
    "mostrar_mejores(tipo1, \"media\", df_merge[columns_media], orden_columnas)\n",
    "tipo1 = 'mul_s'\n",
    "mostrar_mejores(tipo1, \"media\", df_merge[columns_media], orden_columnas)\n",
    "tipo1 = 'mul_m'\n",
    "mostrar_mejores(tipo1, \"media\", df_merge[columns_media], orden_columnas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalización del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.datetime.now()\n",
    "total_time = end_time - start_time\n",
    "myfunc.verbose(f\"Notebook ha tardado {total_time} segundos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
